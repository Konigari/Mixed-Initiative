{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import nltk as nl\n",
    "import xlrd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import openpyxl \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tried using gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for file in os.listdir('./final/'):    \n",
    "    path = './final/' + file\n",
    "    wb_obj = openpyxl.load_workbook(path) \n",
    "    count += 1\n",
    "    sheet_obj = wb_obj.active \n",
    "    m_row = sheet_obj.max_row \n",
    "    \n",
    "      \n",
    "    for i in range(1, m_row + 1): \n",
    "        cell_obj = sheet_obj.cell(row = i, column = 2)\n",
    "        cell_obj1 = sheet_obj.cell(row = i, column = 3) \n",
    "        \n",
    "        if count < 57:\n",
    "                \n",
    "            if cell_obj1.value is None and cell_obj.value is None:\n",
    "                pass\n",
    "            elif cell_obj1.value is None and cell_obj.value is not None:\n",
    "                train_Y.append(0)\n",
    "                train_X.append(cell_obj.value)\n",
    "                \n",
    "            elif cell_obj1.value is not None:\n",
    "                train_X.append(cell_obj.value)\n",
    "                if \"major\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    train_Y.append(1)\n",
    "                elif \"minor\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    train_Y.append(2)\n",
    "                elif \"off\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    train_Y.append(3)\n",
    "                else:\n",
    "                    train_Y.append(0)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            if cell_obj1.value is None and cell_obj.value is None:\n",
    "                pass\n",
    "            elif cell_obj1.value is None and cell_obj.value is not None:\n",
    "                test_Y.append(0)\n",
    "                test_X.append(cell_obj.value)\n",
    "                \n",
    "            elif cell_obj1.value is not None:\n",
    "                test_X.append(cell_obj.value)\n",
    "                \n",
    "                if \"major\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    test_Y.append(1)\n",
    "                elif \"minor\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    test_Y.append(2)\n",
    "                elif \"off\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                    test_Y.append(3)\n",
    "                else:\n",
    "                    test_Y.append(0)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nl.word_tokenize(sentences.lower()) for sentences in train_X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training the word2vec model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tokens, size=300, min_count=1, workers=4)\n",
    "print(\"\\n Training the word2vec model...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45226941, 80449000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(tokens, total_examples=len(tokens), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors   \n",
    "model.wv.save_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh.ramola/venv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/saurabh.ramola/venv/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(model.wv.syn0, train_Y[:4310])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction word2vec : \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0]\n",
      "Score word2vec : \n",
      " 0.965661252900232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh.ramola/venv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/saurabh.ramola/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(model.wv.syn0[:100, :])\n",
    "# Calculating the score of the predictions\n",
    "score = clf.score(model.wv.syn0, train_Y[:4310])\n",
    "print(\"\\nPrediction word2vec : \\n\", predict)\n",
    "print(\"Score word2vec : \\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trying pretrained w2v embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 9664\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "def create_dataset(flag):\n",
    "    count = 0\n",
    "    \n",
    "    for file in os.listdir('./final/'):    \n",
    "        path = './final/' + file\n",
    "        wb_obj = openpyxl.load_workbook(path) \n",
    "        count += 1\n",
    "        sheet_obj = wb_obj.active \n",
    "        m_row = sheet_obj.max_row \n",
    "\n",
    "\n",
    "        for i in range(1, m_row + 1): \n",
    "            cell_obj = sheet_obj.cell(row = i, column = 2)\n",
    "            cell_obj1 = sheet_obj.cell(row = i, column = 3) \n",
    "\n",
    "            if count < 57:\n",
    "\n",
    "                if cell_obj1.value is None and cell_obj.value is None:\n",
    "                    pass\n",
    "                elif cell_obj1.value is None and cell_obj.value is not None:\n",
    "                    y.append(0)\n",
    "                    X.append(nl.word_tokenize(cell_obj.value.lower()))\n",
    "\n",
    "                elif cell_obj1.value is not None:\n",
    "                    if flag:\n",
    "                        X.append(nl.word_tokenize(cell_obj.value.lower()))\n",
    "                        if \"major\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        elif \"minor\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        elif \"off\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        else:\n",
    "                            y.append(0)\n",
    "                    else:\n",
    "                        X.append(nl.word_tokenize(cell_obj.value.lower()))\n",
    "                        if \"major\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        elif \"minor\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        elif \"off\" in cell_obj1.value and \"start\" in cell_obj1.value: \n",
    "                            y.append(1)\n",
    "                        else:\n",
    "                            y.append(0)\n",
    "                    \n",
    "create_dataset(0)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./glove.6B.50d.txt', \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode('utf-8'): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open('./glove.6B.50d.txt', \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        \n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "mult_nb_tfidf       0.9579\n",
      "svc_tfidf           0.9579\n",
      "glove_small         0.9577\n",
      "glove_small_tfidf   0.9575\n",
      "mult_nb             0.9565\n",
      "svc                 0.9496\n",
      "bern_nb             0.9385\n",
      "bern_nb_tfidf       0.9385\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"glove_small\", etree_glove_small),\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = []\n",
    "ay = []\n",
    "def benchmark(model, X, y):\n",
    "    test_size = 0.1\n",
    "    scores = []\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=test_size)\n",
    "    scores.append(accuracy_score(model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "    ax = model.fit(X_train, y_train).predict(X_test)\n",
    "    ay = y_test\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = []\n",
    "for name, model in all_models:\n",
    "    table.append({'model': name, \n",
    "                  'accuracy': benchmark(model, X, y)})\n",
    "df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = svc.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'saw', 'about', 'them', 'traveling', 'to', 'san', 'francisco', 'to', 'see', 'whether', 'they', 'wanted', 'ten', 'foot', 'wide', 'or', 'twelve', 'foot', 'wide', 'cars', 'or', 'something', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['oh', ',', 'you', 'bet', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['uh-huh', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['oh', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['uh-huh', ',']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['uh-huh', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['well', ',', 'i', 'think', 'there', \"'s\", 'a', ',', 'a', 'struggle', 'within', 'the', 'soviet', 'union', '.']\n",
      "Correct ==>  1\n",
      "Predicted ==>  1\n",
      "['i', ',', 'i', 'have', 'a', 'pie', 'crust', 'recipe', 'that', 'i', ',', 'that', 'is', 'unique', ',', 'that', 'has', 'vinegar', 'and', 'egg', 'in', 'it', ',']\n",
      "Correct ==>  1\n",
      "Predicted ==>  1\n",
      "['so', ',', 'we', 'managed', 'to', ',', 'uh', ',', 'figure', 'out', 'how', 'to', 'get', 'at', 'just', 'about', 'everything', 'else', ',', 'but', ',', 'uh', ',', 'but', 'not', ',', 'uh', ',', 'not', 'seafood', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['and', 'they', ',', 'they', 'said', 'that', ',', 'that', 'what', 'they', 'were', 'doing', 'was', 'scraping', 'it', 'with', 'bulldozers', 'and', 'stuff', ',', 'and', 'taking', 'it', 'to', ',', 'uh', ',', 'these', 'like', 'ponds', 'that', 'they', 'had', 'to', 'filter', 'it', 'with', ',']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['because', 'no', 'matter', 'what', 'i', 'did', 'or', 'how', 'i', 'did', 'it', ',', 'i', 'invariably', 'got', 'myself', 'wet', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['well', ',', 'of', 'course', ',', 'you', 'know', ',', 'we', 'have', 'the', 'national', 'embarrassment', 'in', 'our', 'state', ',', 'jessie', 'helms', ',']\n",
      "Correct ==>  1\n",
      "Predicted ==>  1\n",
      "['seriously', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['but', 'i', 'teach', 'for', 'dallas', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['um', ',', 'every', 'now', 'and', 'then', 'i', \"'m\", 'loaned', 'a', 'tape', 'i', 'can', 'stick', 'in', 'the', ',', 'uh', ',', 'in', 'the', 'car', 'cassette', 'set', 'on', 'the', 'way', 'home', 'to', 'make', 'the', 'drive', 'more', 'enjoyable', ',', 'talking', 'about', ',', 'uh', ',', 'better', 'outlooks', 'on', 'things', 'and', 'the', 'philosophy', 'of', ',', 'of', 'pat', 'hagerty', 'and', 'these', 'kind', 'of', ',', 'uh', ',', 'mind', 'stimulating', 'philosophy', 'type', '.', 'which', 'all', ',', 'you', 'know', ',', 'betters', 'yourself', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['that', \"'s\", 'good', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['fully', 'auto', ',']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['yes', ',']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['no', '.']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['before', 'i', 'got', 'married', ',', 'you', 'know', ',', 'i', 'was', 'working']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n",
      "['but', 'it', 'put', 'me', 'in', 'a', 'really', ',', 'it', 'put', 'me', 'in', 'a', 'really', 'bad', 'mood', ',']\n",
      "Correct ==>  1\n",
      "Predicted ==>  1\n",
      "['what', ',']\n",
      "Correct ==>  0\n",
      "Predicted ==>  1\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for word in arr:\n",
    "    if(word):\n",
    "        print(X_test[index])\n",
    "        print(\"Correct ==> \", y_test[index])\n",
    "        print(\"Predicted ==> \", word)\n",
    "        index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
