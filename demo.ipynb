{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.model_type import MODELS\n",
    "from utils.data_utils import DatasetVectorizer\n",
    "from utils.other_utils import init_config\n",
    "from utils.other_utils import logger\n",
    "\n",
    "\n",
    "dirs = ['./Annotation/Saurabh','./Annotation/Rachna + K','./Annotation/Kaveri','./Annotation/Zubair','./Annotation/Saujas']\n",
    "\n",
    "\n",
    "QUESTIONS = [\"qy\", \"qw\", \"qo\", \"qr\",\"qh\"]\n",
    "\n",
    "def get_similarity(sentence1, sentence2):\n",
    "    return(cosine(model.encode(sentence1)[0], model.encode(sentence2)[0]))\n",
    "    \n",
    "    # similarity_processor.get_similarity(sentence1, sentence2)\n",
    "\n",
    "\n",
    "def is_question(utterance):\n",
    "    try:\n",
    "        if utterance.split(\"|\")[2].strip(\" \") in QUESTIONS: \n",
    "            return 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSiameseNetGuiDemo:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.resultLabel = {'Result' : 0}\n",
    "\n",
    "\n",
    "        self.main_config = init_config()\n",
    "        self.model_dir = str(self.main_config['DATA']['model_dir'])\n",
    "\n",
    "        model_dirs = [os.path.basename(x[0]) for x in os.walk(self.model_dir)]\n",
    "\n",
    "\n",
    "        self.vectorizer = DatasetVectorizer(self.model_dir)\n",
    "\n",
    "        self.max_doc_len = self.vectorizer.max_sentence_len\n",
    "        self.vocabulary_size = self.vectorizer.vocabulary_size\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.model = self.load_model1(\"rnn_64\")\n",
    "\n",
    "    def predict(self,sentences1,sentences2):\n",
    "        if self.model:\n",
    "            sentence1 = sentences1\n",
    "            sentence2 = sentences2\n",
    "            x1_sen = self.vectorizer.vectorize(sentence1)\n",
    "            x2_sen = self.vectorizer.vectorize(sentence2)\n",
    "            feed_dict = {self.model.x1: x1_sen, self.model.x2: x2_sen,\n",
    "                         self.model.is_training: False}\n",
    "            prediction = np.squeeze(self.session.run([self.model.predictions], feed_dict=feed_dict))\n",
    "            prediction = np.round(prediction, 2)\n",
    "            self.resultLabel['text'] = prediction\n",
    "            return prediction\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        logger.info('Loading model: %s', model_name)\n",
    "\n",
    "        model = MODELS[model_name.split('_')[0]]\n",
    "        model_config = init_config(model_name.split('_')[0])\n",
    "\n",
    "        self.model = model(self.max_doc_len, self.vocabulary_size, self.main_config, model_config)\n",
    "        saver = tf.train.Saver()\n",
    "        last_checkpoint = tf.train.latest_checkpoint('{}/{}'.format(self.model_dir, model_name))\n",
    "        saver.restore(self.session, last_checkpoint)\n",
    "        logger.info('Loaded model from: %s', last_checkpoint)\n",
    "        saver.save(self.session, 'my_test_model',global_step=1000)\n",
    "        pos = 0\n",
    "        count = 0\n",
    "        for x in dirs: \n",
    "            for file in os.listdir(x):\n",
    "                f = open(x+'/'+file,'r')\n",
    "                print(f.name)\n",
    "                lines =f.readlines()\n",
    "                scan_range = 10\n",
    "                for index, utterance in enumerate(lines):\n",
    "                    if index < scan_range or index >= len(lines) - scan_range:\n",
    "                        continue\n",
    "                    if is_question(utterance):\n",
    "                        a1 = []\n",
    "                        a2 = []\n",
    "                        for i in range(-scan_range,0):\n",
    "                            if len((lines[index+i]).split(\"|\")[1]) > 0:\n",
    "                                a1.append(self.predict(lines[index].split(\"|\")[1], lines[index+i].split(\"|\")[1]))\n",
    "                        for i in range(1,scan_range+1):\n",
    "                            if len((lines[index+i]).split(\"|\")[1]) > 0:\n",
    "                                a2.append(self.predict(lines[index].split(\"|\")[1], lines[index+i].split(\"|\")[1]))\n",
    "#                         a1 = [x for x in a1 if x > 0.68]\n",
    "#                         a2 = [x for x in a2 if x > 0.68]\n",
    "                        keys = ['sbd','sb-d','sd','sw-d']\n",
    "                        if sum(a1) - sum(a2) > 0.3  :\n",
    "                            count += 1\n",
    "                            if 's' in lines[index].split(\"|\")[3].strip(\" \"):\n",
    "                                pos += 1\n",
    "                            else:\n",
    "                                print(\"Predicted wrong Sentence : \" + lines[index] + \" Predicted Label : sbd\")\n",
    "\n",
    "                        if sum(a2) - sum(a1) > 0.2 :\n",
    "                            count += 1\n",
    "                            if 'i' in lines[index].split(\"|\")[3].strip(\" \").lower():\n",
    "                                pos += 1\n",
    "                            else:\n",
    "                                print(\"Predicted wrong Sentence : \" + lines[index] + \" Predicted Label : i\")\n",
    "        print(pos/count)   \n",
    "        \n",
    "    def load_model1(self, model_name):\n",
    "            logger.info('Loading model: %s', model_name)\n",
    "\n",
    "            model = MODELS[model_name.split('_')[0]]\n",
    "            model_config = init_config(model_name.split('_')[0])\n",
    "\n",
    "            self.model = model(self.max_doc_len, self.vocabulary_size, self.main_config, model_config)\n",
    "            saver = tf.train.Saver()\n",
    "            last_checkpoint = tf.train.latest_checkpoint('{}/{}'.format(self.model_dir, model_name))\n",
    "            saver.restore(self.session, last_checkpoint)\n",
    "            logger.info('Loaded model from: %s', last_checkpoint)\n",
    "            saver.save(self.session, 'my_test_model',global_step=1000)\n",
    "            \n",
    "            f = open('./2020.txt','r')\n",
    "            print(f.name)\n",
    "            lines =f.readlines()\n",
    "            scan_range = 5\n",
    "            topics = []\n",
    "            newLines = []\n",
    "            for index, utterance in enumerate(lines):\n",
    "                if index < scan_range or index >= len(lines) - scan_range:\n",
    "                    continue\n",
    "                \n",
    "                if len(lines[index].split(\"|\")[1].split()) > 3:\n",
    "                    a1 = []\n",
    "                    a2 = []\n",
    "\n",
    "                    for i in range(-scan_range,0):\n",
    "                        if len((lines[index+i]).split(\"|\")[1].split()) > 3:\n",
    "                            a1.append(self.predict(lines[index].split(\"|\")[1], lines[index+i].split(\"|\")[1]))\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    for i in range(1,scan_range+1):\n",
    "                        if len((lines[index+i]).split(\"|\")[1].split()) > 3:\n",
    "                            a2.append(self.predict(lines[index].split(\"|\")[1], lines[index+i].split(\"|\")[1]))\n",
    "                        else:\n",
    "                            continue\n",
    "            \n",
    "                    maxVal = -10000000\n",
    "                    indexTopic = -1\n",
    "                    if sum(a1) - sum(a2) > 0.2:\n",
    "                        print(\"a1,a2\",sum(a1),sum(a2))\n",
    "                        print(topics)\n",
    "                        \n",
    "                        if len(topics) < 1:\n",
    "                            topics.append(index)\n",
    "                        else:\n",
    "                            for x in topics:\n",
    "                                temp = self.predict(lines[index].split(\"|\")[1],lines[x].split(\"|\")[1])\n",
    "                                if temp > maxVal:\n",
    "                                    maxVal = temp\n",
    "                                    indexTopic = x\n",
    "                            if maxVal > 0.8:\n",
    "                                print(\"Subdialogue of \\n\",lines[index],lines[indexTopic])\n",
    "                                print(maxVal)\n",
    "                    if sum(a2) - sum(a1) > 0.9:\n",
    "                        topics.append(index)\n",
    "                        print(\"Initiation : \", lines[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading main configuration.\n",
      "INFO:tensorflow:Loading model: rnn_64\n",
      "INFO:tensorflow:Reading configuration for rnn model.\n",
      "WARNING:tensorflow:From /home/saurabh/LTRC_work/3_2/NEWSTATS/sim/multihead-siamese-nets-master/layers/recurrent.py:13: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/saurabh/LTRC_work/3_2/NEWSTATS/sim/multihead-siamese-nets-master/layers/recurrent.py:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model_dir/rnn_64/model-6380\n",
      "INFO:tensorflow:Loaded model from: model_dir/rnn_64/model-6380\n",
      "./2020.txt\n",
      "WARNING:tensorflow:From /home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:203: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "a1,a2 2.0399999916553497 1.7300000190734863\n",
      "[]\n",
      "a1,a2 1.4500000029802322 0.8700000029057264\n",
      "[7]\n",
      "a1,a2 1.570000022649765 0.4700000062584877\n",
      "[7]\n",
      "a1,a2 2.0000000298023224 0.6199999935925007\n",
      "[7]\n",
      "Initiation :  B|I, I do, I do listen to a lot of,|sd\n",
      "\n",
      "Initiation :  B|you know, I do, I switch the stations a lot because I don't have a cassette player in my car.|sd\n",
      "\n",
      "a1,a2 2.229999989271164 1.8300000131130219\n",
      "[7, 16, 17]\n",
      "a1,a2 1.8799999952316284 1.219999983906746\n",
      "[7, 16, 17]\n",
      "Subdialogue of \n",
      " B|You know, I, I, I like music that is, that I feel|sd\n",
      " B|I, I do, I do listen to a lot of,|sd\n",
      "\n",
      "0.83\n",
      "a1,a2 1.8600000143051147 0.78999999538064\n",
      "[7, 16, 17]\n",
      "a1,a2 2.0799999833106995 0.8599999807775021\n",
      "[7, 16, 17]\n",
      "a1,a2 0.6499999985098839 0.3499999940395355\n",
      "[7, 16, 17]\n",
      "a1,a2 0.6000000070780516 0.03999999910593033\n",
      "[7, 16, 17]\n",
      "Initiation :  B|the one thing I do object to about rap music is, is when it becomes militant, or, if it's, uh, violence oriented.|sd\n",
      "\n",
      "Initiation :  B|I, I have strong objections to that.|sd\n",
      "\n",
      "a1,a2 1.7299999743700027 1.2400000095367432\n",
      "[7, 16, 17, 38, 41]\n",
      "Subdialogue of \n",
      " B|one time I remember, this was back when, even, uh,|sd\n",
      " B|I, I have strong objections to that.|sd\n",
      "\n",
      "0.84\n",
      "a1,a2 1.4800000339746475 0.6399999856948853\n",
      "[7, 16, 17, 38, 41]\n",
      "a1,a2 1.419999971985817 0.41999998688697815\n",
      "[7, 16, 17, 38, 41]\n",
      "a1,a2 0.8199999928474426 0.4099999964237213\n",
      "[7, 16, 17, 38, 41]\n",
      "Initiation :  B|And, uh, it was about,|sd\n",
      "\n",
      "a1,a2 1.2799999713897705 0.8800000101327896\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 1.7200000286102295 0.6099999994039536\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 1.1099999770522118 0.6200000047683716\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 0.6700000092387199 0.10000000149011612\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 1.179999977350235 0.6400000136345625\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 1.3599999845027924 0.1699999999254942\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "Subdialogue of \n",
      " A|Do you remember that, that song.|qy\n",
      " B|I, I have strong objections to that.|sd\n",
      "\n",
      "0.8\n",
      "a1,a2 1.1400000005960464 0.12000000290572643\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 0.550000011920929 0.3100000023841858\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "a1,a2 0.8700000047683716 0.2299999985843897\n",
      "[7, 16, 17, 38, 41, 52]\n",
      "Initiation :  B|I think that was a|sd\n",
      "\n",
      "Initiation :  B|I mean it was just like,|sd\n",
      "\n",
      "a1,a2 2.400000035762787 2.1900000125169754\n",
      "[7, 16, 17, 38, 41, 52, 98, 102]\n",
      "a1,a2 1.0999999791383743 0.8500000238418579\n",
      "[7, 16, 17, 38, 41, 52, 98, 102]\n",
      "a1,a2 2.2400000393390656 1.9599999785423279\n",
      "[7, 16, 17, 38, 41, 52, 98, 102]\n",
      "Subdialogue of \n",
      " B|you know, oh, that was really great.|^q\n",
      " B|And, uh, it was about,|sd\n",
      "\n",
      "0.84\n",
      "a1,a2 2.050000011920929 1.0900000035762787\n",
      "[7, 16, 17, 38, 41, 52, 98, 102]\n"
     ]
    }
   ],
   "source": [
    "gui = MultiheadSiameseNetGuiDemo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
